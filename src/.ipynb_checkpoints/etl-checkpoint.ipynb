{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import getcwd\n",
    "from sklearn.experimental import enable_iterative_imputer # https://stackoverflow.com/a/56738037\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 420420\n",
    "DATA_PATH = \"../data/DREAM_data\"\n",
    "TRAIN_PATH = DATA_PATH + '/training'\n",
    "EVAL_PATH = DATA_PATH + '/evaluation'\n",
    "FILENAME_LIST = ['condition_occurrence.csv', 'device_exposure.csv', 'goldstandard.csv', \n",
    "    'measurement.csv', 'observation_period.csv', 'observation.csv', \n",
    "    'person.csv', 'procedure_occurrence.csv', 'visit_occurrence.csv']\n",
    "FILENAME_CLIN_CONCEPT_MAP = { # maps str->list(str)\n",
    "    'condition_occurrence.csv': ['condition_concept_id',\n",
    "                                'condition_type_concept_id',\n",
    "                                'condition_source_concept_id',\n",
    "                                'condition_status_concept_id'],\n",
    "    'device_exposure.csv': ['device_concept_id',\n",
    "                            'device_type_concept_id',\n",
    "                            'device_source_concept_id'],\n",
    "    'measurement.csv': ['measurement_concept_id',\n",
    "                        'measurement_type_concept_id',\n",
    "                        'operator_concept_id',\n",
    "                        'value_as_concept_id',\n",
    "                        'unit_concept_id',\n",
    "                        'measurement_source_concept_id'],\n",
    "    'observation.csv': ['observation_concept_id',\n",
    "                        'observation_type_concept_id',\n",
    "                        'value_as_concept_id',\n",
    "                        'qualifier_concept_id',\n",
    "                        'unit_concept_id',\n",
    "                        'observation_source_concept_id'],\n",
    "    'observation_period.csv': ['period_type_concept_id'],\n",
    "    'procedure_occurrence.csv': ['procedure_concept_id',\n",
    "                                'procedure_type_concept_id',\n",
    "                                'modifier_concept_id',\n",
    "                                'procedure_source_concept_id'],\n",
    "    'visit_occurrence.csv': ['visit_concept_id',\n",
    "                        'visit_type_concept_id',\n",
    "                        'visit_source_concept_id',\n",
    "                        'admitting_source_concept_id',\n",
    "                        'discharge_to_concept_id']}\n",
    "DATA_DICT_DF = pd.read_csv(DATA_PATH + '/data_dictionary.csv').loc[:, ['concept_id', 'concept_name', 'table']]\n",
    "CONCEPT_ID_TO_NAME_MAP = DATA_DICT_DF.loc[:, ['concept_id', 'concept_name']].set_index('concept_id').to_dict()['concept_name']\n",
    "CONCEPT_ID_TO_TABLE_MAP = DATA_DICT_DF.loc[:, ['concept_id', 'table']].set_index('concept_id').to_dict()['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csvs_to_dataframe_dict(fn_list=FILENAME_LIST, path=TRAIN_PATH):\n",
    "    \"\"\"\n",
    "    Loads csvs into single dictionary data structure\n",
    "    :returns: dict (str->DataFrame)\n",
    "    \"\"\"\n",
    "    fn_to_df_dict = {}\n",
    "    for fn in fn_list:\n",
    "        try:\n",
    "            df = pd.read_csv(path + '/' + fn)\n",
    "            fn_to_df_dict[fn] = df\n",
    "        except:\n",
    "            raise ValueError(f\"Error: could not read file: {path+'/'+fn}\")\n",
    "    return fn_to_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_data_univariate(X, missing_val=np.nan, strategy='most_frequent'):\n",
    "    \"\"\"\n",
    "    Imputes missing values in X using univariate approaches\n",
    "    :param X: DataFrame\n",
    "    :param missing_val: int or np.nan\n",
    "    :param strategy: str in {'most_frequent', 'mean', 'median'} OR any numeric (impute constant)\n",
    "    :returns: X with missing_val imputed\n",
    "    \"\"\"\n",
    "    # sklearn docs: https://scikit-learn.org/stable/modules/impute.html#impute\n",
    "    # handle constant impute case\n",
    "    val = None\n",
    "    if type(strategy) != str:\n",
    "        try:\n",
    "            val = float(strategy)\n",
    "            strategy = 'constant'\n",
    "        except:\n",
    "            raise ValueError(f\"Error: parameter 'strategy' needs to be string or numeric, got: {strategy}\")\n",
    "\n",
    "    # build imputer, then apply the transform\n",
    "    imp = SimpleImputer(missing_values=missing_val, strategy=strategy, fill_value=val)\n",
    "    X_new = imp.fit_transform(X)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_data_multivariate(X, missing_val=np.nan, strategy='most_frequent', max_iter=5):\n",
    "    \"\"\"\n",
    "    Imputes missing values in X using multivariate approach (MICE)\n",
    "        NOTE: there are more possible parameters to tweak, though for simplicity focusing on a few to start\n",
    "    :param X: DataFrame\n",
    "    :param missing_val: int or np.nan\n",
    "    :param strategy: str in {'most_frequent', 'mean', 'median', 'constant'}\n",
    "    :param max_iter: int\n",
    "    :returns: X with missing_val imputed\n",
    "    \"\"\"\n",
    "    # sklearn docs: https://scikit-learn.org/stable/modules/impute.html#impute\n",
    "    # MICE paper: https://www.jstatsoft.org/article/view/v045i03\n",
    "\n",
    "    # handle passed constant case\n",
    "    if type(strategy) in {int, float}:\n",
    "        strategy='constant'\n",
    "\n",
    "    # build imputer, then apply transform\n",
    "    imp = IterativeImputer(random_state=RANDOM_SEED, missing_values=missing_val, \\\n",
    "        initial_strategy=strategy, max_iter=max_iter)\n",
    "    X_new = imp.fit_transform(X)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_pid_list(path=TRAIN_PATH):\n",
    "    \"\"\"\n",
    "    Gets unique list of patient IDs from person.csv\n",
    "    :returns: list\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path + '/person.csv')\n",
    "    pid_list = df['person_id'].unique().tolist()\n",
    "    return pid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_pid_pairs(path=TRAIN_PATH):\n",
    "    \"\"\"\n",
    "    Gets all (non-unique) concept_id-person_id pairs as a DataFrame with the following columns:\n",
    "        person_id\n",
    "        concept_id\n",
    "    To get unique, use pd.DataFrame.drop_duplicates after calling this function\n",
    "    :returns: DataFrame\n",
    "    \"\"\"\n",
    "    # use map to quickly access copy of df\n",
    "    fn_to_df_map = load_csvs_to_dataframe_dict(path=path)\n",
    "\n",
    "    # Get all person_id, concept_id occurrences (duplicates included) as a DataFrame\n",
    "    ## estimate df size (assume worst case each person has 1 instance of every concept)\n",
    "    person_count = len(fn_to_df_map['person.csv']['person_id'])\n",
    "    clin_concept_count = 0\n",
    "    for fn in FILENAME_CLIN_CONCEPT_MAP:\n",
    "        df = fn_to_df_map[fn]\n",
    "        for col in FILENAME_CLIN_CONCEPT_MAP[fn]:\n",
    "            clin_concept_count += len(df[col].unique())\n",
    "    ## init df_all\n",
    "    idx = range(person_count * clin_concept_count)\n",
    "    cols = ['person_id', 'concept_id']\n",
    "    df_all = pd.DataFrame(index=idx, columns=cols)\n",
    "    ## populate df_all (unique person_id per concept_id)\n",
    "    count = 0\n",
    "    for fn in FILENAME_CLIN_CONCEPT_MAP:\n",
    "        df = fn_to_df_map[fn]\n",
    "        for col in FILENAME_CLIN_CONCEPT_MAP[fn]:\n",
    "            # pre-process: get all person_id, concept_id pairs (non-unique)\n",
    "            df_sliced = df.loc[:, ['person_id', col]]\n",
    "            df_sliced = df_sliced.dropna()\n",
    "            df_sliced = df_sliced.rename(columns={col: 'concept_id'})\n",
    "            # set appropriate index\n",
    "            n_rows = len(df_sliced)\n",
    "            idx = pd.Series(range(count, count+n_rows))\n",
    "            df_sliced = df_sliced.set_index(idx) \n",
    "            # append to df_all\n",
    "            df_all.iloc[idx, :] = df_sliced\n",
    "            count += n_rows\n",
    "    ## remove NaN\n",
    "    df_all = df_all.dropna().astype('int')\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_concept_summary(path=TRAIN_PATH, save_csv=True):\n",
    "    \"\"\"\n",
    "    Gets a summary of concept_id-person_id pairs as a DataFrame with the following columns:\n",
    "        concept_id\n",
    "        concept_name (if in data_dictionary.csv)\n",
    "        avg_per_pid (if a patient had the concept, how many instances were there)\n",
    "        from_table (if in data_dictionary.csv)\n",
    "        unique_pid_count (how many unique patients had the concept)\n",
    "    :creates_file: concept_summary.csv\n",
    "    :returns: DataFrame\n",
    "    \"\"\"\n",
    "    # get all concept_id-person_id pairs\n",
    "    df_all = get_concept_pid_pairs(path)\n",
    "\n",
    "    # Get count of unique person_id per concept_id\n",
    "    df_all_summary = df_all.drop_duplicates(keep='first')\\\n",
    "        .groupby(['concept_id'])\\\n",
    "        .agg({'person_id': 'count'})\\\n",
    "        .rename(columns={'person_id': 'unique_pid_count'})\\\n",
    "        .sort_values('unique_pid_count', ascending=False)\n",
    "    ## add concept_name, table labels from data_dict\n",
    "    ### names\n",
    "    concept_ids = df_all_summary.reset_index().loc[:, 'concept_id']\n",
    "    cid_to_name = lambda cid: CONCEPT_ID_TO_NAME_MAP[cid] if cid in CONCEPT_ID_TO_NAME_MAP else pd.NA\n",
    "    concept_names = concept_ids.apply(cid_to_name).rename('concept_name')\n",
    "    concept_ids_with_names = pd.concat([concept_ids, concept_names], axis=1).set_index('concept_id')\n",
    "    df_all_summary.insert(0, \"concept_name\", concept_ids_with_names)\n",
    "    ### table\n",
    "    cid_to_table = lambda cid: CONCEPT_ID_TO_TABLE_MAP[cid] if cid in CONCEPT_ID_TO_TABLE_MAP else pd.NA\n",
    "    concept_table = concept_ids.apply(cid_to_table).rename('from_table')\n",
    "    concept_ids_with_table = pd.concat([concept_ids, concept_table], axis=1).set_index('concept_id')\n",
    "    df_all_summary.insert(1, \"from_table\", concept_ids_with_table)\n",
    "\n",
    "    # Get count of average # of occurrences per person with the given concept_id\n",
    "    m = len(df_all)\n",
    "    df_ones = pd.DataFrame(np.ones((m, 1)))\n",
    "    df_all_w_ones = pd.concat([df_all, df_ones], axis=1)\n",
    "    df_all_avg = df_all_w_ones.groupby(['concept_id', 'person_id'])\\\n",
    "        .agg(['sum'])\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={0: 'avg_per_pid'})\\\n",
    "        .groupby(['concept_id'])\\\n",
    "        .agg(['mean'])\n",
    "    ## clean-up formatting a bit\n",
    "    df_all_avg.columns.droplevel([1,2]) # remove multiindex\n",
    "    df_all_avg = df_all_avg.loc[:, 'avg_per_pid'] # keep only avg_per_pid\n",
    "    ## add to df_all_summary\n",
    "    df_all_summary.insert(1, \"avg_per_pid\", df_all_avg)\n",
    "\n",
    "    # save to csv\n",
    "    if save_csv:\n",
    "        df_all_summary.to_csv(DATA_PATH + '/concept_summary.csv')\n",
    "\n",
    "    return df_all_summary.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_list_ordered_by_sparsity(path=TRAIN_PATH, least_sparse_first=True):\n",
    "    \"\"\"\n",
    "    Gets unique list of concept_ids from concept_summary.csv ordered by unique_pid_count, avg_per_pid\n",
    "    :returns: list (sorted)\n",
    "    \"\"\"\n",
    "    # get df (each row = concept)\n",
    "    df = generate_concept_summary(path, save_csv=False)\n",
    "    df_test = df.copy()\n",
    "    # sort dataframe\n",
    "    asc = not least_sparse_first\n",
    "    df_sorted = df.sort_values(['unique_pid_count', 'avg_per_pid'], axis=0, ascending=asc)\n",
    "    return df_test, df_sorted.loc[:, 'concept_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_concept_feature_id_map(concept_id_list):\n",
    "    \"\"\"\n",
    "    Generates dict mapping concept_id to feature_id (0-indexed)\n",
    "    :return: dict (str->int)\n",
    "    \"\"\"\n",
    "    if type(concept_id_list) != list:\n",
    "        raise TypeError(f\"Error: concept_id_list needs to be a list, got: {type(concept_id_list)}\")\n",
    "    # using: https://stackoverflow.com/a/36460020\n",
    "    return {k: v for v, k in enumerate(concept_id_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create feature vectors. give option to impute\n",
    "def create_feature_df(pid_list, path=TRAIN_PATH, impute_strategy=0.0, use_multivariate_impute=False):\n",
    "    \"\"\"\n",
    "    Generates the feature DataFrame of shape m x n, with m=len(pid_list) and n=# of features\n",
    "        NOTE: the value of each cell is the _count_ of a feature for a given patient.\n",
    "            Need to add additional OMOP-specific logic to parse-out values for concepts with values (e.g. heart rate)\n",
    "        If a patient is missing a feature, the given inpute_strategy will be used (univariate unless use_multivariate_impute=True)\n",
    "        By default this pulls all concepts sourced from csv columns specified in FILENAME_CLIN_CONCEPT_MAP\n",
    "    :param pid_list: list of person IDs\n",
    "    :param impute_strategy: str in {'most_frequent', 'mean', 'median'} OR any numeric (impute constant)\n",
    "    #deprecated (takes forever, also doesn't make sense) :param use_multivariate_impute: bool\n",
    "    #TODO implement this :param custom_concept_id_set: set (whitelist)\n",
    "    #TODO implement this :param exclude_concept_id_set: set (blacklist)\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    # helper function\n",
    "    def normalize_array(x):\n",
    "        \"\"\" Given numpy array, return with the each column normalized \"\"\"\n",
    "        # from: https://stackoverflow.com/a/29661707\n",
    "        x_normed = x / x.max(axis=0)\n",
    "        return x_normed\n",
    "\n",
    "    # handle input edge cases\n",
    "    if len(pid_list) == 0:\n",
    "        raise ValueError(f\"Error: pid_list has length 0, got: {pid_list}\")\n",
    "    # if type(custom_concept_id_set) == list:\n",
    "    #     custom_concept_id_set = set(custom_concept_id_set)\n",
    "    # if type(exclude_concept_id_set) == list:\n",
    "    #     exclude_concept_id_set = set(exclude_concept_id_set)\n",
    "\n",
    "    # get concept indices, person list, and concept_id->feature_id dict\n",
    "    df_test, concept_id_list = get_concept_list_ordered_by_sparsity(path=path)\n",
    "    pid_list = get_unique_pid_list(path=path)\n",
    "    concept_feature_id_map = generate_concept_feature_id_map(concept_id_list)\n",
    "\n",
    "    # get all unique person_id, concept_id pairs as DataFrame\n",
    "    df_all = get_concept_pid_pairs(path=path)\n",
    "\n",
    "    # for each person_id, get corresponding concept_id, count pairs\n",
    "    ## get counts\n",
    "    tot = len(df_all)\n",
    "    df_ones = pd.DataFrame(np.ones((tot, 1)))\n",
    "    df_all_w_ones = pd.concat([df_all, df_ones], axis=1)\n",
    "    df_all_summed = df_all_w_ones.groupby(['concept_id', 'person_id'])\\\n",
    "        .agg(['sum'])\\\n",
    "        .reset_index()\\\n",
    "        .set_index('person_id')\n",
    "    ## remove column multiindex, rename column\n",
    "    df_all_summed.columns = df_all_summed.columns.droplevel([1])\n",
    "    df_all_summed = df_all_summed.rename(columns={df_all_summed.columns[1]: 'sum'})\n",
    "    \n",
    "    # generate matrix (rows=person_id, cols=feature_id, values=sum)\n",
    "    get_feature_id = lambda cid: concept_feature_id_map[cid]\n",
    "    rows = df_all_summed.index.to_list()\n",
    "    cols = df_all_summed.loc[:, 'concept_id'].apply(get_feature_id).to_list()\n",
    "    vals = df_all_summed.loc[:, 'sum'].to_list()\n",
    "    m = len(pid_list)\n",
    "    n = len(concept_id_list)\n",
    "    df_sparse = coo_matrix((vals, (rows, cols)), shape=(m, n))\n",
    "    arr_dense = df_sparse.toarray()\n",
    "    \n",
    "    # impute data\n",
    "    # if use_multivariate_impute:\n",
    "    #     arr_imputed = impute_missing_data_multivariate(arr_dense, missing_val=0.0, strategy=impute_strategy)\n",
    "\n",
    "    arr_imputed = impute_missing_data_univariate(arr_dense, missing_val=0.0, strategy=impute_strategy)\n",
    "\n",
    "    # normalize data\n",
    "    arr_norm = normalize_array(arr_imputed)\n",
    "\n",
    "    return concept_id_list, df_test, pid_list, concept_feature_id_map, df_all_summed, df_sparse, arr_dense, arr_imputed, arr_norm, pd.DataFrame(arr_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/anaconda3/envs/Dream/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/mohammad/anaconda3/envs/Dream/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/mohammad/anaconda3/envs/Dream/lib/python3.6/site-packages/pandas/core/generic.py:3887: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "/home/mohammad/anaconda3/envs/Dream/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/mohammad/anaconda3/envs/Dream/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "pid_list = get_unique_pid_list(path=TRAIN_PATH)\n",
    "concept_id_list_train, df_test_train, pid_list_train, concept_feature_id_map_train, df_all_summed_train, df_sparse_train, arr_dense_train, arr_imputed_train, arr_norm_train, X_train = create_feature_df(pid_list, path=TRAIN_PATH, impute_strategy=0.0, use_multivariate_impute=False)\n",
    "# gs = pd.read_csv('../data/DREAM_data/training/goldstandard.csv')\n",
    "# Y_train = gs.drop(['person_id'], axis = 1)\n",
    "# X_train = np.array(X_train)\n",
    "# Y_train = np.array(Y_train).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/anaconda3/envs/Dream/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/mohammad/anaconda3/envs/Dream/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/mohammad/anaconda3/envs/Dream/lib/python3.6/site-packages/pandas/core/generic.py:3887: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "/home/mohammad/anaconda3/envs/Dream/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/mohammad/anaconda3/envs/Dream/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "pid_list_eval = get_unique_pid_list(path=EVAL_PATH)\n",
    "concept_id_list_eval, df_test_eval, pid_list_eval, concept_feature_id_map_eval, df_all_summed_eval, df_sparse_eval, arr_dense_eval, arr_imputed_eval, arr_norm_eval, X_test = create_feature_df(pid_list_eval, path=EVAL_PATH, impute_strategy=0.0, use_multivariate_impute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = df_all_summed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept_id</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>257</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>257</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>257</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>257</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>257</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>46273652</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>46273652</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>46273652</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>46273652</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>46273652</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133402 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           concept_id  sum\n",
       "person_id                 \n",
       "0                 257  1.0\n",
       "1                 257  3.0\n",
       "2                 257  2.0\n",
       "3                 257  1.0\n",
       "5                 257  2.0\n",
       "...               ...  ...\n",
       "451          46273652  1.0\n",
       "453          46273652  1.0\n",
       "475          46273652  1.0\n",
       "500          46273652  1.0\n",
       "520          46273652  1.0\n",
       "\n",
       "[133402 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_summed_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_not_in_eval = [x for x in concept_id_list_train if x not in concept_id_list_eval] \n",
    "eval_not_in_train = [x for x in concept_id_list_eval  if x not in concept_id_list_train] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2461</th>\n",
       "      <th>2462</th>\n",
       "      <th>2463</th>\n",
       "      <th>2464</th>\n",
       "      <th>2465</th>\n",
       "      <th>2466</th>\n",
       "      <th>2467</th>\n",
       "      <th>2468</th>\n",
       "      <th>2469</th>\n",
       "      <th>2470</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.860697</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.698492</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.432836</td>\n",
       "      <td>0.432161</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.870647</td>\n",
       "      <td>0.879397</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>0.393035</td>\n",
       "      <td>0.391960</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.819095</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.824121</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>536 rows × 2471 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.860697  0.859296  0.549020  0.766667  0.549020  0.500000  0.500000   \n",
       "1    0.940299  0.949749  0.509804  0.616667  0.509804  0.615385  0.652174   \n",
       "2    0.701493  0.698492  0.607843  1.000000  0.607843  0.480769  0.478261   \n",
       "3    0.432836  0.432161  0.392157  0.316667  0.392157  0.307692  0.326087   \n",
       "4    0.870647  0.879397  0.686275  0.650000  0.686275  0.750000  0.782609   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "531  0.825871  0.829146  0.745098  0.816667  0.745098  0.615385  0.608696   \n",
       "532  0.393035  0.391960  0.450980  0.433333  0.450980  0.423077  0.456522   \n",
       "533  0.820896  0.819095  0.686275  0.650000  0.686275  0.711538  0.760870   \n",
       "534  0.825871  0.824121  0.647059  0.716667  0.647059  0.596154  0.586957   \n",
       "535  0.865672  0.869347  0.647059  0.750000  0.647059  0.788462  0.804348   \n",
       "\n",
       "         7     8         9     ...  2461  2462  2463  2464  2465  2466  2467  \\\n",
       "0    0.666667   1.0  0.705882  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1    0.488889   1.0  0.549020  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2    0.777778   1.0  0.823529  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3    0.311111   1.0  0.294118  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4    0.577778   1.0  0.647059  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "..        ...   ...       ...  ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "531  0.733333   1.0  0.764706  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "532  0.288889   1.0  0.372549  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "533  0.444444   1.0  0.647059  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "534  0.600000   1.0  0.705882  ...   0.0   0.0   0.0   1.0   0.0   0.0   0.0   \n",
       "535  0.622222   1.0  0.725490  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "     2468  2469  2470  \n",
       "0     0.0   0.0   0.0  \n",
       "1     0.0   0.0   0.0  \n",
       "2     0.0   0.0   0.0  \n",
       "3     0.0   0.0   0.0  \n",
       "4     0.0   0.0   0.0  \n",
       "..    ...   ...   ...  \n",
       "531   0.0   0.0   0.0  \n",
       "532   0.0   0.0   0.0  \n",
       "533   0.0   0.0   0.0  \n",
       "534   0.0   0.0   0.0  \n",
       "535   0.0   0.0   0.0  \n",
       "\n",
       "[536 rows x 2471 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_out = clf_lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = lr_out.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = lr_out.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dream",
   "language": "python",
   "name": "dream"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
